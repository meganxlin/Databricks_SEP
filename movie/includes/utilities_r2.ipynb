{"cells":[{"cell_type":"code","source":["%run ./configuration"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"153bc2c2-d6eb-4b75-acd8-7414a16f8057"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.session import SparkSession\nfrom urllib.request import urlretrieve\nfrom pyspark.sql.functions import from_unixtime, dayofmonth, month, hour\nfrom pyspark.sql import DataFrame\nfrom delta import DeltaTable\nfrom datetime import datetime\nimport time\nimport pandas as pd\nimport json\nfrom typing import List\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.types import StringType\nfrom pyspark.sql.functions import (\n    col,\n    current_timestamp,\n    from_json,\n    from_unixtime,\n    lag,\n    lead,\n    lit,\n    mean,\n    stddev,\n    max,\n    explode\n)\n\nCLASSIC_DATA = \"classic_data_2020_h1.snappy.parquet\"\nCLASSIC_DELTA = \"classic_data_2020_h1.delta\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"29f6e164-549a-438a-9049-82efb6eb5c2d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def retrieve_data(file: str, landingPath: str) -> bool:\n    \"\"\"Download file from remote location to driver. Move from driver to DBFS.\"\"\"\n\n    base_url = \"https://files.training.databricks.com/static/data/health-tracker/\"\n    url = base_url + file\n    driverPath = \"file:/databricks/driver/\" + file\n    dbfsPath = landingPath + file\n    urlretrieve(url, file)\n    dbutils.fs.mv(driverPath, dbfsPath)\n    return True\n  "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f871d80c-9015-48b7-84c9-e232dac3e0e6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def prepare_activity_data(landingPath) -> bool:\n    retrieve_data(CLASSIC_DATA, landingPath)\n\n    classicIngest = (\n        spark.read.format(\"parquet\")\n        .load(landingPath + CLASSIC_DATA)\n        .withColumn(\"time\", from_unixtime(\"time\"))\n        .select(\n            \"*\",\n            dayofmonth(\"time\").alias(\"day\"),\n            month(\"time\").alias(\"month\"),\n            hour(\"time\").alias(\"hour\"),\n        )\n        .write.format(\"delta\")\n        .save(landingPath + CLASSIC_DELTA)\n    )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b09842bd-8290-4ba4-b2e9-1abe08b6ad77"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def ingest_classic_data(hours: int = 1) -> bool:\n    CLASSIC_DELTA = \"classic_data_2020_h1.delta\"\n    classicDelta = spark.read.format(\"delta\").load(landingPath + CLASSIC_DELTA)\n\n    next_batch = classicDelta.orderBy(\"month\", \"day\", \"hour\").limit(10 * hours)\n    file_name = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n\n    (next_batch.write.format(\"json\").save(rawPath + file_name))\n\n    # move file out of directory and rename\n    new_json_file = [\n        file.path for file in dbutils.fs.ls(rawPath + file_name) if \"part\" in file.path\n    ][0]\n    dbutils.fs.mv(new_json_file, rawPath + file_name + \".txt\")\n    dbutils.fs.rm(rawPath + file_name, recurse=True)\n\n    classicIngest = DeltaTable.forPath(spark, landingPath + CLASSIC_DELTA)\n\n    delete_match = \"\"\"\n        ingest.name = next.name AND\n        ingest.time = next.time\n    \"\"\"\n\n    (\n        classicIngest.alias(\"ingest\")\n        .merge(next_batch.alias(\"next\"), delete_match)\n        .whenMatchedDelete()\n        .execute()\n    )\n\n    return True"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"81176e06-2041-4f46-83d1-f8421202ab8d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def untilStreamIsReady(namedStream: str, progressions: int = 3) -> bool:\n    queries = list(filter(lambda query: query.name == namedStream, spark.streams.active))\n    while len(queries) == 0 or len(queries[0].recentProgress) < progressions:\n        time.sleep(5)\n        queries = list(filter(lambda query: query.name == namedStream, spark.streams.active))\n    print(\"The stream {} is active and ready.\".format(namedStream))\n    return True"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"920153dd-f869-476d-a2d0-084a6006347e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def ingest_batch_raw(Path: str) -> DataFrame:\n    return spark.read.format(\"json\").option(\"multiline\", \"true\").option(\"inferSchema\", \"true\").load(Path)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5a7dcb7f-a1a9-4671-b5e8-70d135e36674"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def read_stream_raw(spark: SparkSession, rawPath: str) -> DataFrame:\n  schema = \"value STRING\"\n  return (\n    spark.readStream\n    .format(\"text\")\n    .schema(schema)\n    .load(rawPath)\n  )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3463b53f-b2b8-42c0-92a6-77a6a62be4d2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def transform_raw(df: DataFrame) -> DataFrame:\n  return (\n    df.select(\n      lit(\"files.training.databricks.com\").alias(\"datasource\"),\n      current_timestamp().alias(\"ingesttime\"),\n      \"value\",\n      current_timestamp().cast(\"date\").alias(\"p_ingestdate\")\n    )\n  )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2c6fa11a-63e4-4e4c-9e7e-dbd8f951b538"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def batch_writer(\n    dataframe: DataFrame,\n    partition_column: str,\n    exclude_columns: List = [],\n    mode: str = \"overwrite\",\n) -> DataFrame:\n    return (\n        dataframe.drop(\n            *exclude_columns\n        )  # This uses Python argument unpacking (https://docs.python.org/3/tutorial/controlflow.html#unpacking-argument-lists)\n        .write.format(\"delta\")\n        .mode(mode)\n        .option(\"overwriteSchema\", \"true\")\n        .partitionBy(partition_column)\n    )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1189c932-3f0b-4b2b-9d79-1a4963ff310f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e207a74a-d186-4b73-89e6-27594e1b2a0c"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"utilities","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":1430919566441904}},"nbformat":4,"nbformat_minor":0}
