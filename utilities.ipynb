{"cells":[{"cell_type":"code","source":["\nfrom pyspark.sql.session import SparkSession\nfrom urllib.request import urlretrieve\nfrom pyspark.sql.functions import from_unixtime, dayofmonth, month, hour\nfrom delta import DeltaTable\nfrom datetime import datetime\nimport time\n\nCLASSIC_DATA = \"classic_data_2020_h1.snappy.parquet\"\nCLASSIC_DELTA = \"classic_data_2020_h1.delta\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"29f6e164-549a-438a-9049-82efb6eb5c2d"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def retrieve_data(file: str, landingPath: str) -> bool:\n    \"\"\"Download file from remote location to driver. Move from driver to DBFS.\"\"\"\n\n    base_url = \"https://files.training.databricks.com/static/data/health-tracker/\"\n    url = base_url + file\n    driverPath = \"file:/databricks/driver/\" + file\n    dbfsPath = landingPath + file\n    urlretrieve(url, file)\n    dbutils.fs.mv(driverPath, dbfsPath)\n    return True\n\n\ndef prepare_activity_data(landingPath) -> bool:\n    retrieve_data(CLASSIC_DATA, landingPath)\n\n    classicIngest = (\n        spark.read.format(\"parquet\")\n        .load(landingPath + CLASSIC_DATA)\n        .withColumn(\"time\", from_unixtime(\"time\"))\n        .select(\n            \"*\",\n            dayofmonth(\"time\").alias(\"day\"),\n            month(\"time\").alias(\"month\"),\n            hour(\"time\").alias(\"hour\"),\n        )\n        .write.format(\"delta\")\n        .save(landingPath + CLASSIC_DELTA)\n    )\n\n\ndef ingest_classic_data(hours: int = 1) -> bool:\n    CLASSIC_DELTA = \"classic_data_2020_h1.delta\"\n    classicDelta = spark.read.format(\"delta\").load(landingPath + CLASSIC_DELTA)\n\n    next_batch = classicDelta.orderBy(\"month\", \"day\", \"hour\").limit(10 * hours)\n    file_name = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n\n    (next_batch.write.format(\"json\").save(rawPath + file_name))\n\n    # move file out of directory and rename\n    new_json_file = [\n        file.path for file in dbutils.fs.ls(rawPath + file_name) if \"part\" in file.path\n    ][0]\n    dbutils.fs.mv(new_json_file, rawPath + file_name + \".txt\")\n    dbutils.fs.rm(rawPath + file_name, recurse=True)\n\n    classicIngest = DeltaTable.forPath(spark, landingPath + CLASSIC_DELTA)\n\n    delete_match = \"\"\"\n        ingest.name = next.name AND\n        ingest.time = next.time\n    \"\"\"\n\n    (\n        classicIngest.alias(\"ingest\")\n        .merge(next_batch.alias(\"next\"), delete_match)\n        .whenMatchedDelete()\n        .execute()\n    )\n\n    return True\n\n\ndef untilStreamIsReady(namedStream: str, progressions: int = 3) -> bool:\n    queries = list(filter(lambda query: query.name == namedStream, spark.streams.active))\n    while len(queries) == 0 or len(queries[0].recentProgress) < progressions:\n        time.sleep(5)\n        queries = list(filter(lambda query: query.name == namedStream, spark.streams.active))\n    print(\"The stream {} is active and ready.\".format(namedStream))\n    return True\n  \n  \n  def ingest_batch_raw(Path: str) -> DataFrame:\n    return spark.read.format(\"json\").option(\"multiline\", \"true\").option(\"inferSchema\", \"true\").load(Path)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f871d80c-9015-48b7-84c9-e232dac3e0e6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"<div class=\"ansiout\"><span class=\"ansi-cyan-fg\">  File </span><span class=\"ansi-green-fg\">&#34;&lt;tokenize&gt;&#34;</span><span class=\"ansi-cyan-fg\">, line </span><span class=\"ansi-green-fg\">73</span>\n<span class=\"ansi-red-fg\">    def ingest_batch_raw(Path: str) -&gt; DataFrame:</span>\n    ^\n<span class=\"ansi-red-fg\">IndentationError</span><span class=\"ansi-red-fg\">:</span> unindent does not match any outer indentation level\n</div>","errorSummary":"<span class=\"ansi-red-fg\">IndentationError</span><span class=\"ansi-red-fg\">:</span> unindent does not match any outer indentation level","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-cyan-fg\">  File </span><span class=\"ansi-green-fg\">&#34;&lt;tokenize&gt;&#34;</span><span class=\"ansi-cyan-fg\">, line </span><span class=\"ansi-green-fg\">73</span>\n<span class=\"ansi-red-fg\">    def ingest_batch_raw(Path: str) -&gt; DataFrame:</span>\n    ^\n<span class=\"ansi-red-fg\">IndentationError</span><span class=\"ansi-red-fg\">:</span> unindent does not match any outer indentation level\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8ad10ae0-c1b2-4797-8fd3-ef628a9ac9cc"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"utilities","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":1430919566441904}},"nbformat":4,"nbformat_minor":0}
